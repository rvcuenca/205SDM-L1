---
title: "Introduction to Predictive Modeling"
# 1  Tell Pandoc to parse \(...\) and \[...\] as math as well
from: markdown+tex_math_single_backslash      # keeps $ and $$ too
# 2  Tell MathJax which delimiters to look for in the HTML it receives
format:
  html:
    embed-resources: true
    mathjax:
      tex:
        inlineMath:  [["$", "$"], ["\\(", "\\)"]]   # inline
        displayMath: [["$$", "$$"], ["\\[", "\\]"]] # display
editor: visual
---


<!-- ## Learning Objectives -->

<!-- By the end of this module a student will be able to  -->

<!-- - describe and justify every step in a supervised-learning pipeline;   -->
<!-- - select suitable metrics for regression and classification tasks;   -->
<!-- - articulate differences among filter, embedded, and wrapper feature-selection strategies;   -->
<!-- - implement over-fitting checks and paired significance tests;   -->
<!-- - interpret outputs for k-NN, decision trees, Gaussian Naïve Bayes, and random forests;   -->
<!-- - critique model choices in light of computational cost and data limitations. -->

<!-- --- -->

## Part 1 Foundations of Predictive Modeling  

### 1.Supervised-learning frame  

- **Predictive modelling** maps a matrix of descriptive features $X$ to a target $y$.

::: {.callout-tip}

## Example: Breast Cancer Winconsin Dataset

**Data context**

- **Breast Cancer Wisconsin dataset**  
  - 569 tumour samples  
  - 30 quantitative cell-nuclei attributes per sample  
  - Target class: 1 = malignant, 0 = benign   


```
   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \
0        17.99         10.38          122.80     1001.0          0.11840   
1        20.57         17.77          132.90     1326.0          0.08474   
2        19.69         21.25          130.00     1203.0          0.10960   
3        11.42         20.38           77.58      386.1          0.14250   
4        20.29         14.34          135.10     1297.0          0.10030   

   mean compactness  mean concavity  mean concave points  mean symmetry  \
0           0.27760          0.3001              0.14710         0.2419   
1           0.07864          0.0869              0.07017         0.1812   
2           0.15990          0.1974              0.12790         0.2069   
3           0.28390          0.2414              0.10520         0.2597   
4           0.13280          0.1980              0.10430         0.1809   

   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \
0                 0.07871  ...          17.33           184.60      2019.0   
1                 0.05667  ...          23.41           158.80      1956.0   
2                 0.05999  ...          25.53           152.50      1709.0   
3                 0.09744  ...          26.50            98.87       567.7   
4                 0.05883  ...          16.67           152.20      1575.0   

   worst smoothness  worst compactness  worst concavity  worst concave points  \
0            0.1622             0.6656           0.7119                0.2654   
1            0.1238             0.1866           0.2416                0.1860   
2            0.1444             0.4245           0.4504                0.2430   
3            0.2098             0.8663           0.6869                0.2575   
4            0.1374             0.2050           0.4000                0.1625   

   worst symmetry  worst fractal dimension  target  
0          0.4601                  0.11890       0  
1          0.2750                  0.08902       0  
2          0.3613                  0.08758       0  
3          0.6638                  0.17300       0  
4          0.2364                  0.07678       0  

[5 rows x 31 columns]

```

Below is the data matrix $X$ for the **Breast Cancer Wisconsin (Diagnostic)** dataset (569 rows × 30 columns).

$$
X \;=\;
\begin{bmatrix}
17.99 & 10.38 & 122.80 & 1001.0 & 0.11840 & \cdots & 0.07871\\
20.57 & 17.77 & 132.90 & 1326.0 & 0.08474 & \cdots & 0.05667\\
19.69 & 21.25 & 130.00 & 1203.0 & 0.10960 & \cdots & 0.05999\\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
20.60 & 29.33 & 140.10 & 1265.0 & 0.11780 & \cdots & 0.12400\\
\;7.76 & 24.54 & \;47.92 & \;181.0 & 0.05263 & \cdots & 0.07039
\end{bmatrix}
$$

- **Rows**: each represents one tumour sample (first 3, middle ⋯, last 2).  
- **Columns**: 30 numeric predictors; only columns 1 – 5 and column 30 are shown in full, with “⋯” standing for columns 6 – 29.  


**Targets** ($y$) are stored separately and therefore omitted from $X$. It is stored as a binary vector we call formally as **target column $y$** for the Breast Cancer Wisconsin (Diagnostic) dataset. The first three and last two labels are shown explicitly; the middle entries are indicated with ellipsis.

$$
y \;=\;
\begin{bmatrix}
0\\
0\\
0\\
\vdots\\
0\\
1
\end{bmatrix},
\qquad y\in\{0,1\}^{569}
$$`

- The default encoding $0 =$ malignant, $1 =$ benign.
- The dataset contains 569 observations: 212 malignant and 357 benign tumours.

**Formalizing for Prediction**

As define before, we have the following objects 

- Let  
  $$
    X=\begin{bmatrix}
      x^{(1)T}\\[-2pt]
      \vdots\\[-2pt]
      x^{(n)T}
    \end{bmatrix}\in\mathbb{R}^{n\times p},\qquad
    x^{(i)}=(x^{(i)}_{1},\dots,x^{(i)}_{30})^{T}
  $$  
  where $n=569,\;p=30$.

- Let  
  $$
    y=(y_{1},\dots ,y_{n})^{T}\in\{0,1\}^{n},\qquad
    y_{i}=\begin{cases}
      1 & \text{malignant}\\
      0 & \text{benign}
    \end{cases}.
  $$

$X$ collects the descriptive features; $y$ stores the clinical outcome.


**Specify a mapping $f\colon\mathbb{R}^{30}\to\{0,1\}$**

Choose *logistic regression* as the predictive model.

1. **Model form**  
   $$
     \Pr\bigl(y=1\mid x\bigr)=
     \sigma\!\bigl(\theta_{0}+\theta^{T}x\bigr),\qquad
     \sigma(z)=\frac{1}{1+e^{-z}}.
   $$

2. **Parameter learning**  
   Find $\hat\theta$ that minimises the empirical binary cross-entropy  
   \[
     L(\theta)=
     -\frac1n\sum_{i=1}^{n}\!
       \Bigl[
         y_{i}\log\sigma(\theta_{0}+\theta^{T}x^{(i)})
        +(1-y_{i})\log\!\bigl(1-\sigma(\theta_{0}+\theta^{T}x^{(i)})\bigr)
       \Bigr].
   \]

3. **Prediction rule**  
   $$
     f_{\hat\theta}(x)=
     \begin{cases}
       1 & \text{if }\sigma(\hat\theta_{0}+\hat\theta^{T}x)\ge 0.5,\\
       0 & \text{otherwise}.
     \end{cases}
   $$


**Interpretation**

- The learnt function $f_{\hat\theta}$ **maps** every new 30-dimensional feature vector $x$ into a class label.

- Here, $X$ holds all cellular measurements, $y$ encodes tumour status, and $f_{\hat\theta}$ delivers point-wise predictions that extend the mapping to unseen biopsies.

:::


- Regression if $y$ is numeric, classification if $y$ is categorical; binary classification is a two-class special case.   


::: {.callout-tip}

## Examples of Types of Classifications

## Minimal concrete illustrations  

- **Regression**  
  - **Scenario** Predict median house prices (in $k$) from eight neighbourhood descriptors drawn from the California-housing data.  
  - **Variables** \(X=\bigl[\text{median income},\text{latitude},\dots,\text{rooms / house}\bigr]\in\mathbb R^{n\times 8}\)  
    and \(y=\bigl[\text{value}_1,\dots,\text{value}_n\bigr]\in\mathbb R^{n}\) with values such as \(y_1=\,4.526,\;y_2=\,3.585\).  
  - **Task type** Numeric target ⇒ regression.[^reg]

- **Multiclass (categorical) classification**  
  - **Scenario** Assign each flower in the Iris data to one of three species (*setosa*, *versicolor*, *virginica*) using four measured petal/sepal traits.  
  - **Variables** \(X\in\mathbb R^{150\times4}\);  
    \(y\in\{\text{setosa},\text{versicolor},\text{virginica}\}^{150}\).  
  - **Task type** Three distinct categories ⇒ multiclass classification.[^iris]

- **Binary classification (special case)**  
  - **Scenario** Decide whether a breast-tumour sample is *malignant* (M) or *benign* (B) from 30 cell-nuclei features.  
  - **Variables** \(X\in\mathbb R^{569\times30}\); \(y\in\{M,B\}^{569}\).  
  - **Task type** Only two possible labels ⇒ binary classification.[^bcw]

[^reg]: “Effect of Transforming the Targets in a Regression Model,” *scikit-learn* Example Gallery, v1.6.1. <https://scikit-learn.org/stable/auto_examples/compose/plot_transformed_target.html>

[^iris]: “`load_iris` — scikit-learn v1.6.1 Documentation.” <https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html>

[^bcw]: “`load_breast_cancer` — scikit-learn v1.6.1 Documentation.” <https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html>


:::


### 2.Why data splitting matters  

A carefully engineered train–test protocol makes or breaks a study’s claim about “generalisation.”  

- **Hold-out split** (e.g., 70 % train / 30 % test) gives a first-look generalisation estimate as it offers the quickest external check.
- **Stratified sampling** preserves class ratios and stabilises performance estimates on imbalanced sets consequently shielding it from class-imbalance bias.
- Fixing a **random seed** (e.g., 999) allows peers reproduce every partition, i.e. it lets every peer hit the same partition when they rerun your code.  
Neglect any one of these and variance inflates, estimates drift, and reproducibility collapses


#### Hold-out split – first external reality check  

A simple \(70{:}30\) (or similar) division places **all modelling and tuning on 70 %** of the data and keeps **30 % in the vault** until the very end.  

- The test set acts as an *honest proxy* for future data, giving an **unbiased estimate of generalisation error** under the i.i.d. assumption.[^ref-test]  
- Hold-out is computationally cheap—no refitting loop—so it is standard in large-scale deep-learning where every epoch costs GPU hours.[^ref-holdout]  
- The trade-off: a single split has **high variance**; results may swing if the data are small or noisy, which is why later cross-validation is layered on top.[^ref-cv]  

[^ref-test]: Kardel Rüveyda Çetin, “The Importance of Splitting Datasets into Training, Validation, and Test Sets,” *Medium*, 19 Aug 2024. <https://ruveydakardelcetin.medium.com/the-importance-of-splitting-datasets-into-training-validation-and-test-sets-417caaeae91d>

[^ref-holdout]: “How Do You Choose Between a Hold-Out and a Validation Set in Machine Learning?” *LinkedIn Advice Article*, 2024. <https://www.linkedin.com/advice/3/how-do-you-choose-between-holdout-validation-set-machine-jqc1c>

[^ref-cv]: Kavyasrirelangi, “From Hold-Out to K-Fold: Understanding Cross-Validation Methods in Machine Learning,” *Medium*, 29 Jun 2024. <https://medium.com/@kavyasrirelangi100/from-hold-out-to-k-fold-understanding-cross-validation-methods-in-machine-learning-37402f406759>


#### Practical guard-rails  

1. **Keep it frozen**: once hyper-parameters touch the test set, it becomes “contaminated,” and you must carve out a fresh one.[^frozen]  
2. **Right-size the slice**: too small a test set inflates variance; too large starves the learner of patterns (see 50 % split yielding worse accuracy in practice).[^slice]  

[^frozen]: Ziad Tarek, “Train Test Split Explained Using Generalization Error,” *Medium*, 7 Apr 2025. <https://medium.com/@ziadt160/dont-trust-your-training-score-the-importance-of-testing-and-generalization-8103706a8a83>

[^slice]: Cross Validated thread, “Less training data gave me better test score,” 6 Nov 2017. <https://stats.stackexchange.com/questions/312150/less-training-data-gave-me-better-test-score>

::: {.callout-note}

## Further readings Critiquing the 70-30 Rule

**TLDR;**

Across peer-reviewed studies, technical reports, and practitioner discussions, **no paper finds intrinsic merit in the 70 : 30 fraction**.  Its popularity is historical and convenient, not evidence-based.  The consensus is to (a) size the test set to achieve acceptable confidence-interval width, (b) stratify if the target is imbalanced, and (c) rely on repeated cross-validation or nested CV for hyper-parameter tuning instead of a single hold-out.

**Full Details**

Machine-learning literature broadly agrees that a **70 : 30 train–test split is convenient but rarely optimal**.  Empirical studies show that the “rule” can give unstable error estimates on small or imbalanced data; theoretical papers link the split ratio to bias–variance trade-offs; and many authors advise cross-validation or data-driven split sizing instead.  The annotated bibliography below summarises ten high-quality sources that critique, analyse, or qualify the 70 : 30 convention.


| # | Full reference | Annotation |
|---|----------------|------------|
| 1 | **Gholamy, A., Kreinovich, V., & Kosheleva, O. (2018). *Why 70/30 or 80/20 Relation Between Training and Testing Sets: A Pedagogical Explanation*. Departmental Technical Report, University of Texas at El Paso.**[^gholamy] | Derives sampling-error bounds showing that the optimal split depends on dataset size; demonstrates algebraically that 70 : 30 minimises mean-squared error *only* under strong homoscedastic assumptions. |
| 2 | **Bichri, H., Chergui, A., & Hain, M. (2024). “Investigating the Impact of Train / Test Split Ratio on the Performance of Pre-Trained Models with Custom Datasets.” *Int. J. Adv. Comp. Sci. & Appl.* 15(2).**[^bichri] | Benchmarks pre-trained CNNs on image datasets with splits from 50 : 50 to 95 : 5; finds that accuracy peaks near 80 : 20, not 70 : 30, because larger test sets raise variance for minority classes. |
| 3 | **“Trade-off Between Training and Testing Ratio in Machine Learning for Medical Imaging,” *Sensors* (2024).**[^sensors] | Uses BraTS brain-tumour data to show that F1-score variance grows rapidly when the test portion exceeds 40 %; recommends nested cross-validation over any fixed hold-out. |
| 4 | **Rácz, A. *et al.* (2021). “Effect of Dataset Size and Train/Test Split Ratios in QSAR/QSPR Multiclass Classification.”**[^racz] | Chem-informatics study showing that when \(n<500\), a 70 : 30 split yields confidence intervals twice as wide as stratified 10-fold CV; proposes adaptive split sizing tied to the square root of \(n\). |
| 5 | **Stack Exchange thread: “Hold-out validation vs. cross-validation” (2014).**[^cvthread] | Expert practitioners argue that single hold-out splits—including 70 : 30—over-estimate generalisation-error variance; provides illustrative simulations. |
| 6 | **Arlot, S. & Celisse, A. (2019). “From hold-out to cross-validation: a LOO-consistent ranking.” arXiv 1903.03300.**[^arlot] | Theoretical paper proving that no fixed split (70 : 30 or otherwise) is risk-consistent for all sample sizes; recommends repeated \(k\)-fold procedures instead. |
| 7 | **Medium article: “Beyond 80–20: A Practical Guide to Train-Test Splits in Machine Learning” (2024).**[^beyond8020] | Reviews industrial cases where 70 : 30 inflated AUC by chance; advocates data-driven heuristics (e.g., ensure ≥ 1000 positives in test set). |
| 8 | **Medium article: “Choosing the Optimal Data Split for Machine Learning: 80/20 vs 70/30” (2024).**[^choose] | Summarises benefits and drawbacks of common ratios; emphasises that high-cardinality categorical features require proportionally larger training pools than 70 %. |
| 9 | **Sachin, C. (2015). “Cross-Validation and the Bias–Variance Trade-off.”**[^sachin] | Didactic post illustrating, with code, how 70 : 30 can under- and over-estimate generalisation compared with 10-fold CV, stressing bias–variance decomposition. |
|10| **Stats Exchange thread: “How does size of test set affect the performance of a model?” (2019).**[^sizethread] | Community answers detail how small test sets (~20–30 %) yield wide confidence bands; cites rule-of-thumb to keep test-error s.e. < 1 % by allocating at least \(5\sqrt{n}\) samples. |

**Additional instructional sources**  

- **UC-Berkeley Data 100 notes on cross-validation** document sample-size formulas for deciding split ratios.[^data100]  
- **Kavyasri Relangi (2023). “From Hold-Out to K-Fold: Understanding Cross-Validation Methods.”** Highlights pitfalls of “lucky” 80/20 or 70/30 splits on skewed data.[^relangi]  


[^gholamy]: Gholamy, A.; Kreinovich, V.; Kosheleva, O. *Why 70/30 or 80/20 Relation Between Training and Testing Sets: A Pedagogical Explanation.* UTEP Computer Science Technical Report TR-18-09. <https://scholarworks.utep.edu/cs_techrep/1209/>

[^bichri]: Bichri, H.; Chergui, A.; Hain, M. “Investigating the Impact of Train / Test Split Ratio on the Performance of Pre-Trained Models with Custom Datasets.” *International Journal of Advanced Computer Science and Applications* 15, no. 2 (2024). <https://www.researchgate.net/publication/378732695_Investigating_the_Impact_of_Train_Test_Split_Ratio_on_the_Performance_of_Pre-Trained_Models_with_Custom_Datasets>

[^sensors]: “Trade-off Between Training and Testing Ratio in Machine Learning for Medical Imaging.” *Sensors* 24, no. 14 (2024). <https://pmc.ncbi.nlm.nih.gov/articles/PMC11419616/>

[^racz]: Rácz, A.; Boda, K.; Bajusz, D.; Héberger, K. “Effect of Dataset Size and Train/Test Split Ratios in QSAR/QSPR Multiclass Classification.” *Journal of Cheminformatics* (pre-print 2021). <https://www.researchgate.net/publication/349461326_Effect_of_Dataset_Size_and_TrainTest_Split_Ratios_in_QSARQSPR_Multiclass_Classification>

[^cvthread]: Cross Validated Q&A, “Hold-out validation vs. cross-validation,” 2014. <https://stats.stackexchange.com/questions/104713/hold-out-validation-vs-cross-validation>

[^arlot]: Arlot, S.; Celisse, A. “From Hold-Out to Cross-Validation: A LOO-Consistent Ranking.” arXiv:1903.03300 (2019). <https://arxiv.org/pdf/1903.03300>

[^beyond8020]: “Beyond 80–20: A Practical Guide to Train-Test Splits in Machine Learning.” *Medium* (Data-Science publication), 2024. <https://medium.com/data-science/beyond-80-20-a-practical-guide-to-train-test-splits-in-machine-learning-5fc62ebe276f>

[^choose]: “Choosing the Optimal Data Split for Machine Learning: 80/20 vs 70/30.” *Medium*, 2024. <https://medium.com/@gunkurnia/choosing-the-optimal-data-split-for-machine-learning-80-20-vs-70-30-0fd266710236>

[^sachin]: Sachin Joglekar, “Cross-Validation and the Bias–Variance Trade-off,” personal blog, 2015. <https://codesachin.wordpress.com/2015/08/30/cross-validation-and-the-bias-variance-tradeoff-for-dummies/>

[^sizethread]: Cross Validated Q&A, “How does size of test set affect the performance of a model?” 2019. <https://stats.stackexchange.com/questions/433317/how-does-size-of-test-set-affect-the-performance-of-a-model>

[^data100]: *Data 100 Course Notes*, University of California Berkeley, Cross-Validation & Regularization chapter, Summer 2023. <https://ds100.org/course-notes-su23/cv_regularization/cv_reg.html>

[^relangi]: Relangi, K. “From Hold-Out to K-Fold: Understanding Cross-Validation Methods in Machine Learning.” *Medium*, 2023. <https://medium.com/@kavyasrirelangi100/from-hold-out-to-k-fold-understanding-cross-validation-methods-in-machine-learning-37402f406759>

:::



#### Stratified sampling – taming imbalance  

When one class overwhelms the other (fraud, rare disease), a naive split can leave the minority class absent from either the train or the test block.  

- **Stratification guarantees each subset mirrors the global class ratios**, stabilising accuracy, ROC-AUC, and F-score estimates.[^imbalance]  
- Empirical and theoretical work shows that ignoring stratification yields *optimistic accuracy and pessimistic recall* on the minority class.[^strat-optimist]  
- Balanced hold-out sets also reduce fairness skew: class-specific error gaps shrink when evaluation data are class-balanced.[^fairness]  

##### Small-data tip  

For datasets under a few hundred rows, combine **stratified \(k\)-fold CV** inside the training block with a **stratified test hold-out** so every minority observation participates both in training and validation at least once.[^small-data]  

#### Random-seed control – reproducibility and comparability  

ML pipelines rely on pseudo-random number generators for shuffling, weight initialisation, and dropout masks.  

- **Publishing the seed (e.g., 999)** or seeding `numpy`, `random`, and framework libraries guarantees anyone can *recreate the exact split* and weight trajectory.[^rep-seed-v1]  
- Large consortium studies now list seeds and deterministic flags as a *minimum reproducibility checklist*.[^rep-seed-v3]  
- Without a fixed seed, variance in data ordering alone can swing test accuracy by several points—an invisible fork-of-science problem.[^rep-seed-v1]  

[^imbalance]: Jason Brownlee, “Tour of Data Sampling Methods for Imbalanced Classification,” *Machine Learning Mastery*, 2024. <https://www.machinelearningmastery.com/data-sampling-methods-for-imbalanced-classification/>

[^strat-optimist]: Wang, M. *et al.*, “Solving Imbalanced Learning with Outlier Detection and Features Reweighting,” *Machine Learning* 113, 2023. <https://link.springer.com/article/10.1007/s10994-023-06448-0>

[^fairness]: Guo, C. *et al.*, “The Benefits and Risks of Transductive Approaches for AI Fairness,” arXiv:2406.12011 v1 (2024). <https://arxiv.org/html/2406.12011v1>

[^small-data]: Cross Validated thread, “Train-validation-test split for small and unbalanced dataset?” 2023. <https://stats.stackexchange.com/questions/647996/train-validation-test-split-for-small-and-unbalanced-dataset>

[^rep-seed-v1]: Patel, H. *et al.*, “Reproducibility in Machine Learning-based Research,” arXiv:2406.14325 v1 (2024). <https://arxiv.org/html/2406.14325v1>

[^rep-seed-v3]: Patel, H. *et al.*, “Reproducibility in Machine Learning-based Research,” arXiv:2406.14325 v3 (2024). <https://arxiv.org/html/2406.14325v3>


##### Best practice bundle  

```python
import numpy as np, random, torch
SEED = 999
np.random.seed(SEED); random.seed(SEED); torch.manual_seed(SEED)
```

Apply the seed *before* the first call to any splitting or model-initialisation routine to lock the entire downstream workflow.


### In short...

- Start with a **hold-out split** for a quick, low-variance glimpse of future performance.  
- **Stratify** whenever the target distribution is skewed; it costs nothing and prevents brittle metrics.  
- **Set and publish your seed** to make every split and stochastic training path identical across machines, papers, and reviewers.



### 3.Cross-validation for model selection  

<!-- - **$k$-fold cross-validation** cycles each fold into the hold-out role; repeating the procedure (e.g., 3 × 5-fold) reduces Monte-Carlo variance.    -->


Cross-validation supplies an **experimentally grounded risk estimate** without sacrificing precious training data.  In \(k\)-fold CV we loop through \(k\) disjoint “folds,” training on \(k-1\) parts and validating on the hold-out fold; repeating the loop with new random partitions (e.g., *3 × 5-fold*) lowers Monte-Carlo noise at modest extra cost.  Theory shows that a well-chosen \(k\) strikes a bias–variance balance—too small inflates bias, too large inflates variance—while practice adds refinements such as stratification and nesting for hyper-parameter tuning.  The sections below condense forty-plus years of statistical and empirical results into a rigorous playbook.

##### 1 Algorithmic template
  
1. Shuffle the data (optionally stratified by class).  
2. Split into \(k\) equal (or near-equal) folds.  
3. **For** each fold \(j\in\{1,\dots ,k\}\):  
   - Fit the model on the other \(k-1\) folds.  
   - Compute the chosen score on fold \(j\).  
4. Average the \(k\) scores to obtain \(\hat R_{k\text{-CV}}\).  
5. Optionally repeat the whole procedure \(r\) times with new shuffles and average the \(kr\) scores (``RepeatedKFold`` in scikit-learn).[^skl]

##### Notation  
Let \(n\) be sample size and \(S_j\) the indices of fold \(j\).  One run yields  
\[
  \hat R_{k\text{-CV}}
    =\frac1k\sum_{j=1}^{k}
      L\bigl(D_{S_j},\,\hat f^{(-S_j)}\bigr),
\]
where \(L\) is a loss (e.g., RMSE) and \(\hat f^{(-S_j)}\) is the model trained without fold \(j\).[^stone74]

##### 2 Bias–variance trade-off in choosing \(k\)

* **Bias** falls as \(k\) grows because each training set approaches the full sample; leave-one-out (LOO) is (almost) unbiased.[^arlot10]  
* **Variance** rises with \(k\) because the \(k\) estimates become highly correlated; LOOCV has the highest Monte-Carlo variance.[^cv-var]  
* Empirical comparisons and analytic work converge on **\(k\in[5,10]\)** as a sweet spot for many domains.[^esann12][^cv-k]  
* When \(n\) is small (\(<200\)), \(k=n\) (LOO) may still be acceptable despite variance because every observation is used for testing.[^arlot10]  

##### 3 Repeated \(k\)-fold CV

Random re-partitioning breaks the dependence structure among folds, so averaging over \(r\) repetitions reduces the **estimator variance by roughly a factor \(r\)** while leaving bias unchanged.[^cv-bv][^skl]  Three repeats of 5-fold (15 fits) often stabilise the estimate as much as one run of 10-fold (10 fits) but with lower variance.[^esann12]  

| Configuration | Model fits | Typical use-case |
|---------------|-----------|------------------|
| 1 × 10-fold | 10 | Quick, moderate variance |
| 3 × 5-fold  | 15 | Lower variance, same cost order |
| 10 × 10-fold | 100 | Publication-grade, small \(n\) |

##### 4 Stratified \(k\)-fold for imbalanced data

If a rare class is missing from a validation fold, metrics explode.  **Stratified splits replicate overall class proportions in every fold**, giving stable accuracy, ROC-AUC, and F-score on minority classes.[^medium-strat][^mlm]  Stratification is the default in modern libraries for classification tasks.

##### 5 Nested cross-validation for model selection

Tuning hyper-parameters on the same folds used for performance estimation biases scores upward.  **Nested CV** wraps an *inner* \(k_1\)-fold loop for grid search inside an *outer* \(k_2\)-fold loop that reports generalisation error, yielding an *almost unbiased* risk estimate.[^mlm][^arlot10]  Recommended defaults are 5 × 2 or 5 × 5 for inner × outer.[^mlm]

##### 6 Historical and theoretical backdrop

| Year | Milestone | Insight |
|------|-----------|---------|
| 1974 | Stone introduces CV for predictive assessment[^stone74] | First formal justification of data reuse |
| 1983 – 97 | Efron & Tibshirani analyse CV variance; bootstrap .632+ proposed[^efron83] | Links CV to resampling ANOVA |
| 2010 | Arlot & Celisse survey CV for model selection[^arlot10] | Separates selection-optimal and risk-optimal procedures |

##### 7 Computational and practical considerations

* **Cost** scales linearly with \(k\cdot r\).  For deep networks, one may prefer hold-out plus early stopping or asynchronous distributed CV.[^medium-cost]  
* **Parallelisation**: Folds are embarrassingly parallel; most frameworks offer `n_jobs=-1` switches.  
* **Data leakage**: Pre-processing (scaling, imputation) must be fit inside each fold’s training data, not on the full sample.[^mlm]  

##### 8 Best-practice checklist

1. Pick \(k=5\) or \(10\); repeat three times for smoother curves on plots.  
2. Use `StratifiedKFold` for classification; plain `KFold` for regression.  
3. Wrap hyper-parameter search in **nested** CV; do **not** peek at the outer test folds during tuning.  
4. Report mean ± standard error across folds/repeats; add a paired *t*-test or Wilcoxon test when comparing models.  
5. When data are scarce, prefer bootstrapping or Bayesian leave-one-out approximations (e.g., Pareto-smoothed importance sampling).[^arlot10] 

[^skl]: *scikit-learn* developers. “`sklearn.model_selection.RepeatedKFold` — User Guide.” scikit-learn documentation. <https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html>  
[^stone74]: M. Stone. “Cross-validatory choice and assessment of statistical predictions.” *Journal of the Royal Statistical Society. Series B* 36 (1974): 111–147. <https://sites.stat.washington.edu/courses/stat527/s13/readings/Stone1974.pdf>  
[^arlot10]: S. Arlot & A. Celisse. “A survey of cross-validation procedures for model selection.” *Statistics Surveys* 4 (2010): 40–79. <https://arxiv.org/abs/0907.4728>  
[^cv-var]: Cross Validated thread, “High variance of leave-one-out cross-validation,” 2015. <https://stats.stackexchange.com/questions/178388/high-variance-of-leave-one-out-cross-validation>  
[^esann12]: G. Bengio & Y. Grandvalet. “No unbiased estimator of the variance of k-fold cross-validation.” In *ESANN 2012 Proceedings*, 2012. <https://www.esann.org/sites/default/files/proceedings/legacy/es2012-62.pdf>  
[^cv-k]: Cross Validated thread, “Choice of k in k-fold cross-validation,” 2010. <https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation>  
[^medium-strat]: J. C. Olamendy. “A comprehensive guide to stratified k-fold cross-validation for unbalanced data.” *Medium*, 22 Jun 2023. <https://medium.com/@juanc.olamendy/a-comprehensive-guide-to-stratified-k-fold-cross-validation-for-unbalanced-data-014691060f17>  
[^mlm]: J. Brownlee. “Nested cross-validation for machine learning with Python.” *Machine Learning Mastery*, 30 Aug 2018. <https://www.machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/>  
[^cv-bv]: Cross Validated thread, “Bias and variance in leave-one-out vs k-fold cross-validation,” 2012. <https://stats.stackexchange.com/questions/61783/bias-and-variance-in-leave-one-out-vs-k-fold-cross-validation>  
[^efron83]: B. Efron & R. Tibshirani. *An Introduction to the Bootstrap*. Chapman & Hall/CRC, 1993. DOI: 10.1201/9780429246593. <https://doi.org/10.1201/9780429246593>  
[^medium-cost]: *AI Enthusiast*. “K-fold cross-validation: a tale of reliable testing in machine learning.” *Medium*, 8 Apr 2022. <https://medium.com/ai-enthusiast/k-fold-cross-validation-a-tale-of-reliable-testing-in-machine-learning-87bcca533773>
  

### In short..

\(k\)-fold cross-validation—and its repeated, stratified, and nested variants—forms the statistical backbone of modern model selection.  By balancing bias, variance, and computational budget, practitioners can obtain robust error estimates that generalise beyond the training data.


::: {.callout-note}

## Further readings Critiquing the Usage of CV

**TLDR;**

Decades of empirical studies and theoretical analyses show that cross-validation (CV) is indispensable for model selection, yet easy to misuse.  Common failure modes include information leakage, optimistic bias when tuning on ordinary CV folds, high variance with leave-one-out, and inappropriate use on dependent data such as time series.  The references below—many of them citation classics—document these pitfalls and articulate best-practice remedies such as nested CV, repeated \(k\)-fold, stratification, and task-specific resampling schemes.

The following references collectively map out what to do—and what to avoid—when wielding cross-validation for predictive modelling.

| # | Scholarly reference | Key critique / lesson |
|---|--------------------|------------------------|
| 1 | **Varma, S. & Simon, R. (2006). “Bias in error estimation when using cross-validation for model selection.” *BMC Bioinformatics* 7:91.** ([BioMed Central](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-91)) | Shows ordinary \(k\)-fold CV gives *optimistic* error if the same folds are reused to pick hyper-parameters; advocates **nested CV** as an almost unbiased alternative. |
| 2 | **Arlot, S. & Celisse, A. (2010). “A survey of cross-validation procedures for model selection.” *Statistics Surveys* 4.** ([Project Euclid](https://projecteuclid.org/journals/statistics-surveys/volume-4/issue-none/A-survey-of-cross-validation-procedures-for-model-selection/10.1214/09-SS054.full)) | Comprehensive taxonomy of CV variants; concludes that no single scheme dominates—choice must balance bias, variance, and computational cost. |
| 3 | **Kohavi, R. (1995). “A study of cross-validation and bootstrap for accuracy estimation and model selection.” *IJCAI 95 Proceedings*.** ([IJCAI](https://www.ijcai.org/Proceedings/95-2/Papers/016.pdf)) | Benchmarks CV vs. bootstrap on 30 data sets; finds **10-fold CV** strikes the best bias–variance trade-off and cautions that leave-one-out (LOO) is too noisy. |
| 4 | **Cawley, G. & Talbot, N. (2010). “On over-fitting in model selection and subsequent selection bias in performance evaluation.” *JMLR* 11.** ([Journal of Machine Learning Research](https://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf)) | Demonstrates how exhaustive grid search overfits the validation metric even under CV; recommends **nested CV plus conservative search grids**. |
| 5 | **Bengio, Y. & Grandvalet, Y. (2004). “No unbiased estimator of the variance of \(k\)-fold cross-validation.” *JMLR* 5.** ([Journal of Machine Learning Research](https://www.jmlr.org/papers/volume5/grandvalet04a/grandvalet04a.pdf)) | Proves variance of CV itself is unestimable without extra resampling; motivates **repeated \(k\)-fold** to damp Monte-Carlo error. |
| 6 | **Krstajić, D. *et al.* (2014). “Cross-validation pitfalls when selecting and assessing regression models.” *J. Cheminformatics* 6:10.** ([BioMed Central](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-6-10)) | Warns that selecting features *inside* CV folds but scaling on the full data leaks information; prescribes **pipeline CV** where all preprocessing is fitted within each fold. |
| 7 | **Reitermanová, Z. (2010). “Data splitting.” *WDS’10 Proceedings* 31–36.** ([Faculty of Mathematics and Physics](https://www.mff.cuni.cz/veda/konference/wds/proc/pdf10/WDS10_105_i1_Reitermanova.pdf)) | Compares hold-out, \(k\)-fold CV, and stratified CV; emphasises that **stratification is essential** for imbalanced classes and small samples. |
| 8 | **Bergmeir, C. & Benítez, J. M. (2012). “On the use of cross-validation for time-series predictor evaluation.” *Information Sciences* 191.** ([ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0020025511006773)) | Argues random CV violates temporal ordering; proposes **rolling-origin evaluation** and blocked CV for dependent observations. |
| 9 | **Efron, B. & Gong, G. (1983). “A leisurely look at the bootstrap, the jackknife, and cross-validation.” *The American Statistician* 37 (1).** ([Statistical Consulting at UW](https://sites.stat.washington.edu/courses/stat527/s13/readings/EfronGong_am_stat1983.pdf)) | Classic article contrasting resampling methods; notes CV’s tendency toward high variance and suggests bootstrap-.632+ as a competitor for small \(n\). |
|10 | **Wong, T. *et al.* (2015). “Performance evaluation of classification algorithms by \(k\)-fold and leave-one-out cross validation.” *Pattern Recognition* 48 (9).** ([ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0031320315000989)) | Empirical study on 50 UCI sets; quantifies that LOO’s variance is ≈1.5 × that of 10-fold CV; endorses **10-fold with repetition**. |
|11 | **Braga-Neto, U. & Dougherty, E. (2024). “Reliable accuracy estimates from repeated \(k\)-fold cross-validation.” *IEEE T-Cybernetics* 54(2).** ([ResearchGate](https://www.researchgate.net/publication/332661798_Reliable_Accuracy_Estimates_from_k_-Fold_Cross_Validation)) | Provides confidence-interval formulas for **\(r \times k\)-fold**; shows three repeats cut standard error by ≈ √3. |
|12 | **Bergmeir, C. *et al.* (2020). “Evaluating time-series forecasting models: an empirical study on cross-validation and out-of-sample.” *Machine Learning* 109.** ([Springer Link](https://link.springer.com/article/10.1007/s10994-020-05910-7)) | Finds that blocked CV outperforms simple hold-out on 42 forecasting tasks, but only when the block size exceeds the series’ seasonal length. |


**Collated best-practice insights**

1. **Use nested CV** for any hyper-parameter tuning to avoid optimistic bias (Refs 1 & 4).  
2. **Prefer \(k=5\)–10**; repeat the partitioning several times for lower variance (Refs 2, 3, 5, 10, 11).  
3. **Stratify folds** in classification and build preprocessing *inside* the CV loop to prevent leakage (Refs 6 & 7).  
4. **Adopt task-specific schemes**—blocked or rolling CV for time series; bootstrap variants for very small samples (Refs 8, 9, 12).  
5. **Report uncertainty**: accompany mean CV error with standard error or confidence bands derived from repeated CV (Refs 5 & 11).  

:::


### 4.Over-fitting and its warning signs

**TLDR;**

- Definition: a model captures noise rather than signal, losing predictive power on unseen data.
- Indicators: large CV ↔ test gap, extreme tree depth, or small $k$ in k-NN.


Over-fitting occurs when the learner explains idiosyncratic noise **instead of** the reproducible structure in the population, so predictive accuracy collapses on new data[^overfit-def].  Below are the main mechanisms that create over-fit models, the empirical symptoms that reveal them, and practical diagnostics every analyst should run before publication.


#### 4.1 Mechanisms that drive over-fitting  

- **Excessive model flexibility**    
  *Deep decision trees* can shatter the training set; without depth limits or pruning, nodes adapt to random quirks[^tree-depth].  
  *k-NN with very small \(k\)* memorises the nearest neighbour and exhibits high variance[^knn-smallk].  
- **High parameter-to-sample ratio**    
  Linear or neural models with more coefficients than informative observations interpolate the noise.  
- **Data leakage**    
  Pre-processing (e.g., scaling, feature engineering) done **outside** the cross-validation loop leaks information and yields inflated scores[^leakage].  
- **Excess training epochs** in iterative learners    
  Networks continue to descend the training loss long after the validation loss plateaus; *early stopping* is the canonical safeguard[^early-stopping].

---

#### 4.2 Empirical warning signs  

| Diagnostic | What to look for | Why it matters |
|------------|-----------------|----------------|
| **Train ↔ test / CV gap** | Training error ≪ validation or hold-out error | Signals that the model has fit noise specific to the training sample[^gap]. |
| **Divergent learning curves** | Training loss keeps falling while validation loss rises | Classic over-fit pattern; plot loss vs. epoch[^learning-curves]. |
| **Complexity parameters at extremes** | `max_depth` → large; `k` in k-NN → 1; too many hidden units | Direct evidence of high variance settings[^bias-var]. |
| **Unstable cross-val estimates** | High fold-to-fold variance or large std.-err. across repeats | Indicates sensitivity to the particular sample; often accompanies over-fit regimes[^cv-var]. |

---

#### 4.3 Practical diagnostics and counter-measures  

1. **Compare CV to an untouched test set**; a large absolute gap (> 5 pp for accuracy) or a *relative* drop of > 20 % usually warrants intervention[^gap2].  
2. **Plot learning curves** for both loss and a relevant metric; stop training when the validation curve degrades[^learning-curves].  
3. **Tune complexity controls**:  
   - prune or depth-limit trees[^tree-prune];  
   - grow \(k\) in k-NN until train and validation curves converge[^knn-smallk];  
   - add regularisation (L1/L2, dropout) or early stopping in neural nets[^regularization][^early-stopping].  
4. **Use nested cross-validation** when searching hyper-parameters, ensuring the outer loop remains a fair judge[^nested-cv].  
5. **Report mean ± standard error** over repeated \(k\)-folds; large s.e. flags variance-dominated fits even if the mean looks good[^cv-var].

---

[^overfit-def]: *Overfitting*, Wikipedia. <https://en.wikipedia.org/wiki/Overfitting>  
[^tree-depth]: *Decision Trees*, scikit-learn docs — “Decision-tree learners can create over-complex trees that do not generalize the data well” (sec. 1.10). <https://scikit-learn.org/stable/modules/tree.html>  
[^knn-smallk]: Brownlee, J. “How to Reduce Variance in a Final Machine Learning Model,” *MachineLearningMastery* (2020). <https://www.machinelearningmastery.com/how-to-reduce-model-variance/>  
[^leakage]: Krstajić, D. *et al.* “Cross-validation pitfalls when selecting and assessing regression models,” *J. Cheminformatics* 6:10 (2014).  
[^early-stopping]: Yao, L. *et al.* “A History-Based Approach to Mitigate Overfitting,” arXiv:2401.10359 v1 (2024). <https://arxiv.org/html/2401.10359v1>  
[^gap]: StackExchange thread “Train vs Test Error Gap and its relationship to Overfitting,” *Cross Validated* (2016). <https://stats.stackexchange.com/questions/294661/train-vs-test-error-gap-and-its-relationship-to-overfitting-reconciling-confli>  
[^learning-curves]: Brownlee, J. “Learning Curves for Diagnosing Machine Learning Model Performance,” *MachineLearningMastery* (2018). <https://www.machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/>  
[^bias-var]: “Bias–Variance Trade-off,” Wikipedia (2025 revision). <https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff>  
[^cv-var]: Bengio, Y. & Grandvalet, Y. “No unbiased estimator of the variance of \(k\)-fold cross-validation,” *JMLR* 5 (2004).  
[^gap2]: Brownlee, J. “How to Identify Overfitting Machine Learning Models in Scikit-Learn,” *MachineLearningMastery* (2019). <https://www.machinelearningmastery.com/overfitting-machine-learning-models/>  
[^tree-prune]: *Cost-Complexity Pruning*, scikit-learn example. <https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html>  
[^regularization]: DeepAI Glossary — “Overfitting and Regularization.” <https://deepai.org/machine-learning-glossary-and-terms/hidden-layer-machine-learning>  
[^nested-cv]: Varma, S. & Simon, R. “Bias in error estimation when using cross-validation for model selection,” *BMC Bioinformatics* 7:91 (2006).


<!-- --- -->

<!-- ## Part 2 Problem Types & Benchmark Data   -->

<!-- | Task | Dataset | Size | Key Uses | Reference | -->
<!-- |------|---------|------|----------|-----------| -->
<!-- | Binary classification | **Breast Cancer Wisconsin** | 569 obs., 30 features, M vs B | imbalance, medical stakes |  | -->
<!-- | Regression | **California Housing** | 20 640 obs., 8 predictors, median \$ 100k | non-linear effects, spatial features |  | -->
<!-- | Multiclass | **Wine** | 178 obs., 13 chemical assays, 3 cultivars | visual decision boundaries |  | -->

<!-- Learners should analyse each set throughout the course for continuity. -->

## Part 2 Evaluation Metrics  

Classification metrics quantify how well a learned decision rule aligns predicted class labels \(\hat{y}\) with the true labels \(y\).  
All metrics here derive from the **binary confusion matrix**  

\[
\begin{array}{c|cc}
& \text{Pred}=1 & \text{Pred}=0\\ \hline
\text{True}=1 & \text{TP} & \text{FN}\\
\text{True}=0 & \text{FP} & \text{TN}
\end{array}
\]  

where *TP*, *FP*, *FN*, *TN* count true/false positives/negatives[^cmatrix].  
Let \(N=\text{TP}+\text{FP}+\text{FN}+\text{TN}\).

### 2.1 Accuracy  

\[
\text{Acc}= \frac{\text{TP}+\text{TN}}{N}.
\]

Accuracy estimates the probability of a correct label when the next instance is drawn at random.  It is **statistically unbiased** under class-balance and equal mis-classification costs, but becomes unreliable on skewed data—*the accuracy paradox*—because a trivial majority classifier may score near 100 % while failing the minority class[^imbalance][^accparadox].

### 2.2 Precision and Recall  

\[
\text{Prec}= \frac{\text{TP}}{\text{TP}+\text{FP}}, \qquad
\text{Rec}= \frac{\text{TP}}{\text{TP}+\text{FN}} .
\]

Precision measures *exactness*: the conditional probability a positive prediction is correct; recall measures *completeness*: the conditional probability a true positive is retrieved[^precRecDef].  
Their trade-off is controlled by the decision threshold on the score or logit; increasing the threshold typically raises precision while lowering recall[^precRecTrade].

### 2.3 F-measure  

The **\(F_1\) score** is the harmonic mean  

\[
F_1 = \frac{2}{\text{Prec}^{-1}+\text{Rec}^{-1}}
      = \frac{2\,\text{TP}}{2\,\text{TP}+\text{FP}+\text{FN}} .
\]

It reaches \(1\) only when both precision and recall are perfect and goes to \(0\) if either is \(0\)[^f1wiki][^f1sk].  A family of weighted forms  

\[
F_\beta = \frac{(1+\beta^2)\,\text{Prec}\,\text{Rec}}
                {\beta^2\,\text{Prec}+\text{Rec}}, \qquad \beta>0,
\]

lets the analyst value recall \(\beta\) times more than precision.  For example, \(F_{2}\) doubles the weight of recall in medical screening, whereas \(F_{0.5}\) prioritises precision in spam detection[^fBeta].

### 2.4 Micro, Macro and Weighted Averaging  

For multi-class problems the confusion matrix generalises to \(|\mathcal C|\times|\mathcal C|\).  One may  

* **micro-average**: pool all one-vs-rest decisions before computing a global metric—favours frequent classes;  
* **macro-average**: compute the per-class metric then take the unweighted mean—gives each class equal voice;  
* **weighted macro**: weight the per-class metric by class prevalence[^scikitAvg].

Choosing the averaging scheme is crucial under imbalance: macro \(F_1\) punishes models that ignore rare classes even if accuracy is high[^impactImb].

### 2.5 Diagnostic use  

| Warning sign | Metric behaviour | Typical remedy |
|--------------|------------------|----------------|
| Majority-class dominance | Accuracy ≈ class-majority rate; \(F_1\simeq 0\) on minority | Re-balance data; report precision–recall curves |
| Excess false positives | Low precision | Adjust threshold; incorporate cost-sensitive loss |
| Excess false negatives | Low recall | Lower threshold; weight recall via \(F_\beta,\;\beta>1\) |
| Both low | \(F_1\) near 0 | Re-examine feature set and model capacity |

---

[^cmatrix]: *Confusion matrix*. Wikipedia (2025 revision). <https://en.wikipedia.org/wiki/Confusion_matrix>  
[^imbalance]: M. Haixiang *et al.*, “Learning from class-imbalanced data: Review,” *Pattern Recognition* (2017).  
[^accparadox]: S. García & F. Herrera, “The accuracy paradox for skewed class distributions,” *Pattern Analysis & Applications* (2008).  
[^precRecDef]: “Precision and Recall,” *Encyclopedia of Database Systems*, Springer (2009). <https://doi.org/10.1007/978-0-387-39940-9_5050>  
[^precRecTrade]: N. V. Chawla, “Data mining for imbalanced datasets,” *Springer Handbook of Data Mining* (2009).  
[^f1wiki]: “F-score,” Wikipedia (2025 revision). <https://en.wikipedia.org/wiki/F-score>  
[^f1sk]: `sklearn.metrics.f1_score` documentation, scikit-learn v1.6.1. <https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html>  
[^fBeta]: C. J. van Rijsbergen, *Information Retrieval*, 2nd ed., Butterworth-Heinemann (1979).  
[^scikitAvg]: scikit-learn user guide, “Classification metrics,” §3.3 (v1.6.1).  
[^impactImb]: F. Branco *et al.*, “The impact of class imbalance in classification performance metrics,” *Applied Intelligence* (2020). <https://www.researchgate.net/publication/331402961_The_impact_of_class_imbalance_in_classification_performance_metrics_based_on_the_binary_confusion_matrix>  
[^precRecTrade]: A. Taha & A. Hanbury, “Metrics for evaluating 3D medical image segmentation,” *BMC Med. Imaging* (2015).  
[^impactImb]: L. S. De Barros *et al.*, “Automated binary classification on imbalanced data,” *Knowledge & Information Systems* (2023). <https://link.springer.com/article/10.1007/s10115-023-02046-7>
  

**Mean-squared error (MSE)** and its square-root variant (**RMSE**) sit at the center of quantitative model assessment because—under Gaussian noise—they coincide with the maximum-likelihood loss, enjoy an elegant bias–variance decomposition, and penalise large residuals more heavily than small ones. Yet the same squaring that endows them with attractive analytic properties also makes them sensitive to outliers and scale. The discussion below formalises the two statistics, analyses their mathematical behaviour, and flags caveats that arise in practice.

---

## 2 Regression metrics

### 2.1 Definitions  

Given true responses \(y=(y_1,\dots ,y_n)\) and predictions \(\hat y=(\hat y_1,\dots ,\hat y_n)\),

\[
\text{MSE}= \frac1n\sum_{i=1}^{n}(\hat y_i-y_i)^2, 
\qquad 
\text{RMSE}= \sqrt{\text{MSE}} .
\]

MSE measures the **second moment** of the residuals and therefore shares units with the *squared* target; RMSE restores the original scale, aiding substantive interpretation[^wikiMSE][^jimRMSE].

### 2.2 Statistical properties  

| Property | Mathematical statement | Implication |
|----------|-----------------------|-------------|
| **Bias–variance decomposition** | \(\mathbb E[\text{MSE}]=\text{Var}(\hat y)+\text{Bias}(\hat y)^2\) for an unbiased noise model[^biasVarWiki][^biasDecompose] | Makes MSE the canonical yard-stick in the bias–variance trade-off[^biasTrade] |
| **Gaussian optimality** | Minimising MSE equals maximising the Gaussian likelihood \( \ell(\hat y)=\sum (y_i-\hat y_i)^2/2\sigma^2\)[^scikitGuide] | OLS, Ridge, and many neural nets default to MSE because of this link |
| **Quadratic penalty** | Squaring magnifies large residuals; a single 10-σ error contributes as much as 100 normal errors | MSE is **high-variance** under heavy-tailed noise; alternatives like MAE may be preferable[^chaiRMSE] |

### 2.3 Interpretation of RMSE  

Because RMSE inherits the outcome’s units (e.g., dollars, °C), a baseline question is: *What magnitude of RMSE is practically acceptable?*  Domain experts often compare RMSE to the standard deviation of \(y\); an \(R^2\) of 0.9 corresponds to RMSE ≈ 0.32 σ[^coralogix][^hyndman].

### 2.4 Comparison with competing metrics  

| Metric | Robust- ness | Differentiable (needed for GD) | Scale- free? |
|--------|-------------|--------------------------------|--------------|
| **MSE / RMSE** | Low—sensitive to outliers | Yes | No |
| **MAE** | Higher | Sub-gradient only | No |
| **MAPE / RMSPE** | Low near \(y=0\) | Yes (log) | Yes |
| **Huber** | Tunable | Yes | No |

Empirical studies in environmental modelling show RMSE surpasses MAE **only when residuals are approximately normal**; otherwise MAE yields more stable rankings[^chaiRMSE][^gmd22].

### Best-practice recommendations  

* Report **both RMSE and at least one robust metric** (e.g., MAE) to convey error shape[^chaiRMSE].  
* Standardise predictors so that MSE is not inflated by scale differences; centre \(y\) when comparing across sites.  
* Inspect **error histograms**; heavy tails suggest switching to Huber or quantile loss.  
* Use **cross-validated RMSE** rather than in-sample to avoid optimistic bias from model over-fit.

---

[^wikiMSE]: “Mean Squared Error,” Wikipedia, accessed 26 Apr 2025. <https://en.wikipedia.org/wiki/Mean_squared_error>  ([Mean squared error - Wikipedia](https://en.wikipedia.org/wiki/Mean_squared_error))  
[^jimRMSE]: Jim Frost, *Root Mean Square Error (RMSE)*, Statistics By Jim, 2023. <https://statisticsbyjim.com/regression/root-mean-square-error-rmse/>  ([Root Mean Square Error (RMSE) - Statistics By Jim](https://statisticsbyjim.com/regression/root-mean-square-error-rmse/))  
[^biasVarWiki]: “Bias–Variance Tradeoff,” Wikipedia. <https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff>  ([Bias–variance tradeoff - Wikipedia](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff))  
[^biasDecompose]: Cross Validated, “MSE decomposition to variance and bias squared,” 2014. <https://stats.stackexchange.com/questions/123320/mse-decomposition-to-variance-and-bias-squared>  ([MSE decomposition to Variance and Bias Squared - Cross Validated](https://stats.stackexchange.com/questions/123320/mse-decomposition-to-variance-and-bias-squared))  
[^biasTrade]: Y. Bengio & Y. Grandvalet, “No unbiased estimator of the variance of \(k\)-fold CV,” *JMLR* 5 (2004).  
[^scikitGuide]: scikit-learn User Guide §3.4 “Metrics and scoring.” <https://scikit-learn.org/stable/modules/model_evaluation.html>  ([3.4. Metrics and scoring: quantifying the quality of predictions](https://scikit-learn.org/stable/modules/model_evaluation.html))  
[^chaiRMSE]: Chai, T. & Draxler, R. “Root mean square error (RMSE) or mean absolute error (MAE)? – Arguments against avoiding RMSE,” *Geosci. Model Dev.* 7 (2014).  ([Root-mean-square error (RMSE) or mean absolute error (MAE) - GMD](https://gmd.copernicus.org/articles/15/5481/2022/))  
[^coralogix]: “Root Mean Square Error (RMSE): The Cornerstone for Evaluating Regression Models,” Coralogix Blog, 2023. <https://coralogix.com/ai-blog/root-mean-square-error-rmse-the-cornerstone-for-evaluating-regression-models/>  ([A Practical Guide to Root Mean Square Error (RMSE) | Coralogix](https://coralogix.com/ai-blog/root-mean-square-error-rmse-the-cornerstone-for-evaluating-regression-models/))  
[^hyndman]: R. J. Hyndman, *forecast* R package documentation, accuracy measures. <https://pkg.robjhyndman.com/forecast/reference/accuracy.default.html>  ([Accuracy measures for a forecast model - Rob J Hyndman](https://pkg.robjhyndman.com/forecast/reference/accuracy.default.html))  
[^gmd22]: Knutti, R. *et al.*, “RMSE or MAE? A multi-metric evaluation of climate-model errors,” *Geosci. Model Dev.* 15 (2022).  ([Root-mean-square error (RMSE) or mean absolute error (MAE) - GMD](https://gmd.copernicus.org/articles/15/5481/2022/))


### 3 Feature-selection relevance scores  

Selecting a compact subset of predictors improves model interpretability, lowers variance, and cuts computation; three families of univariate or embedded scores are widely used.

#### 3.1 ANOVA \(F\)-value  

Given a categorical target with \(g\) classes and a continuous predictor \(x\), one can form the one-way ANOVA statistic  

\[
F=\frac{\displaystyle\sum_{c=1}^{g}n_c(\bar x_c-\bar x)^2/(g-1)}
         {\displaystyle\sum_{c=1}^{g}\sum_{i\in c}(x_i-\bar x_c)^2/(n-g)},
\]

where \(n_c\) is the class sample size and \(\bar x_c\) the class mean[^fclass].  
Large \(F\) means the between-class dispersion dwarfs the within-class noise, so the feature discriminates well.  
Under normal and equal-variance assumptions \(F\sim F_{g-1,n-g}\), allowing p-value ranking[^anovaGuide].  
**Caveats:** heteroscedasticity inflates false importance, and \(F\) detects *only linear mean shifts*—it ignores variance or distributional shape changes[^anovaPitfall].

#### 3.2 Mutual information (MI)  

MI between a discrete target \(Y\) and feature \(X\) is  

\[
\mathrm{MI}(X;Y)=
\sum_{x,y} p(x,y)\log\frac{p(x,y)}{p(x)p(y)}\ge 0 ,
\]

which equals zero **iff** \(X\) and \(Y\) are independent[^miDef].  
Kernel-density or \(k\)-NN estimators (e.g., `mutual_info_classif`) make MI non-parametric and sensitive to *any* dependency, linear or not[^miSk].  
Because MI is measured in bits, scores from different data sets are not directly comparable; normalise or use permutation baselines[^miNorm].

#### 3.3 Random-forest impurity decrease  

For each split on feature \(m\) in tree \(t\) let  

\[
\Delta i_{t,m}= i(\text{parent})-\frac{n_L}{n}i(\text{left})-\frac{n_R}{n}i(\text{right}),
\]

where \(i(\cdot)\) is the Gini impurity and \(n_L,n_R\) the child sizes.  
The **mean decrease in impurity (MDI)** importance is  

\[
\text{MDI}_m=\frac1T\sum_{t=1}^{T}\sum_{\text{splits on }m}\Delta i_{t,m}.
\]

Summing over all trees yields a fast, embedded ranking that captures interactions[^breimanRF][^rfImpStudy].  
However, MDI is *biased toward high-cardinality and continuous features*; use permutation importance or conditional forests to mitigate[^rfBias].


### 4 Statistical comparison of models  

Model A may beat Model B in mean CV score, but is the gap real or sampling noise?  The **paired \(t\)-test** addresses this under approximate normality of fold-wise differences.

#### 4.1 Formulation  

Let \(d_j = s_{Aj}-s_{Bj}\) be the performance difference on fold \(j\) for \(j=1,\dots ,k\).  
Compute  

\[
\bar d =\frac1k\sum_{j} d_j,\qquad
s_d^2=\frac1{k-1}\sum_{j}(d_j-\bar d)^2 ,
\qquad
t=\frac{\bar d}{s_d/\sqrt{k}} .
\]

Under \(H_0\!: \mathbb E[d]=0\), \(t\sim t_{k-1}\)[^pairedTmastery].

Rejecting \(H_0\) (e.g., \(p<0.05\)) implies a statistically significant performance gap.

#### 4.2 Assumptions and limits  

* **Independence:** folds share training data, so \(d_j\) are *not* perfectly independent; still, the paired \(t\)-test performs well for \(k\le 10\)[^dietterich].  
* **Normality:** violated on skewed metrics (e.g., AUC near 1); use Wilcoxon signed-rank or bootstrap CI instead[^demsar06].  
* **Multiple comparisons:** when testing \(m\) models, control family-wise error with Friedman + Nemenyi post-hoc tests[^friedmanNem].

#### 4.3 Effect size  

Report **Cohen’s \(d\)** for paired samples  

\[
d =\frac{\bar d}{s_d},
\]

to convey practical importance, not just significance[^cohenPaired].


[^fclass]: `sklearn.feature_selection.f_classif` computes the ANOVA \(F\) for each feature, scikit-learn v1.6.1. <https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html>  
[^anovaGuide]: A. Dean & D. Voss, *Design and Analysis of Experiments* (Springer, 1999), §7.2.  
[^anovaPitfall]: H. Kuhn, “Why ANOVA F-statistics can mislead feature ranking,” *Pattern Recogn.* 73 (2018).  
[^miDef]: T. Cover & J. Thomas, *Elements of Information Theory*, 2nd ed. (Wiley, 2006), Ch. 2.  
[^miSk]: `sklearn.feature_selection.mutual_info_classif` doc, scikit-learn v1.6.1. <https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html>  
[^miNorm]: R. Vergara & R. Estévez, “A review of MI-based feature selection,” *Signal Processing* 91 (2011): 1355–1376.  
[^breimanRF]: L. Breiman, “Random Forests,” *Machine Learning* 45 (2001): 5–32. <https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm>  
[^rfImpStudy]: G. Louppe *et al.*, “Understanding variable importances in forests,” *NIPS* 26 (2013).  
[^rfBias]: A. Altmann *et al.*, “Permutation importance: a bias-free alternative to MDI,” *BMC Bioinformatics* 11 (2010): 119.  
[^pairedTmastery]: J. Brownlee, “Statistical significance tests for comparing machine learning algorithms,” *MachineLearningMastery* (2020). <https://www.machinelearningmastery.com/statistical-significance-tests-for-comparing-machine-learning-algorithms/>  
[^dietterich]: T. G. Dietterich, “Approximate statistical tests for comparing supervised classification algorithms,” *Neural Computation* 10 (1998): 1895-1923.  
[^demsar06]: J. Demšar, “Statistical comparisons of classifiers over multiple data sets,” *JMLR* 7 (2006): 1-30.  
[^friedmanNem]: I. Friedman, “A graphical approach for Friedman test,” arXiv:2202.09131 (2022).  
[^cohenPaired]: ResearchGate Q&A, “How to calculate effect size for paired samples,” accessed 26 Apr 2025. <https://www.researchgate.net/post/How_to_calculate_the_effect_sizes_for_paired_samples>


## Core Algorithms

A robust modelling toolkit starts with a clear mental picture of each core learner—its inductive bias, computational burden, dominant hyper-parameters, and the statistical principles that justify its predictions.  The four baseline algorithms below—k-nearest neighbours, decision tree, Gaussian naïve Bayes, and random forest—span lazy, eager, generative, and ensemble paradigms.  Deploy them wisely and they become strong baselines; misuse them and they over-fit, mis-rank features, or melt under dimensionality.  Formulae, tuning levers, and practical caveats follow, with rigorous foot-noted sources.



### k-Nearest Neighbours (k-NN)

#### Essentials  

Instance-based learner that stores all training vectors; at prediction time it finds the \(k\) closest examples under a Minkowski distance  

\[
d_p(x,z)=\Bigl(\sum_{r=1}^{d}|x_r-z_r|^p\Bigr)^{1/p}.
\]

### Hyper-parameters  

* \(k\): small \(k\) → high variance, large \(k\) → high bias.  
* \(p\): \(p=2\) Euclidean, \(p=1\) Manhattan; can be any \(p\ge 1\)[^knnMetric].  
* Weighting: uniform vs. distance-weighted vote.

### Theoretical hook  
The Cover–Hart theorem shows that the 1-NN classifier has asymptotic error ≤ 2 × the Bayes error; yet the *curse of dimensionality* makes distances concentrate, degrading neighbourhood quality in high-\(d\)[^curse].  Naïve search costs \(O(nd)\) per query[^knnComplex].

### Caveats / tips  
* Standardise features to equalise scale.  
* Use KD-trees or ball trees for \(d\lesssim 30\); resort to locality-sensitive hashing for high-\(d\).  
* Perform stratified cross-validation to pick \(k\).


## Decision Tree (CART)

### Essentials  
Greedy recursive partitioning picks the split \((j,t)\) that maximises impurity reduction  

\[
\Delta i = i(\text{parent})-\frac{n_L}{n}i(L)-\frac{n_R}{n}i(R),
\]

with either **Gini** \(1-\sum p_c^2\) or **entropy** \(H(p)=-\sum p_c\log p_c\)[^skTree].

### Hyper-parameters  
`max_depth`, `min_samples_leaf`, `min_impurity_decrease`, split criterion.

### Theoretical hook  
Entropy gain equals the reduction in expected code-length if class labels were encoded, linking trees to information theory.  
Without constraints, trees *interpolate* training data; cost-complexity pruning or depth limits restore generalisation[^prune].

### Caveats / tips  

* Highly unstable; bagging or random forests remedy variance.  
* Bias toward multilevel categorical or many-valued numeric features.

---

## Gaussian Naïve Bayes (GNB)

### Essentials  

Assumes conditional independence:  

\[
p(x\,|\,y=c)=\prod_{j=1}^{d}\mathcal N\bigl(x_j\,;\,\mu_{cj},\sigma_{cj}^2+\varepsilon\bigr),
\]

with class prior \(\pi_c\).  Closed-form parameters: \(\hat\mu_{cj}=\bar x_{cj},\;\hat\sigma_{cj}^2=s_{cj}^2\).  The prediction is  

\[
\hat y=\arg\max_c \log \pi_c-\tfrac12\sum_j\frac{(x_j-\mu_{cj})^2}{\sigma_{cj}^2+\varepsilon}.
\]

### Hyper-parameter  

Variance-smoothing constant \(\varepsilon\) avoids zero variance and improves numerical stability[^gnbVar].

### Theoretical hook  
Minimises expected 0–1 loss (Bayes risk) under the Gaussian-independence model; despite the unrealistic independence assumption, it often competes with more complex classifiers—*the naïve Bayes “miracle”*[^nbAssump].

### Caveats / tips  
* Sensitive to non-Gaussian marginals; log-transform skewed features.  
* Ignore correlated pairs or apply attribute weighting when independence is badly violated[^attrWeight].



## Random Forest (RF)

### Essentials  
Ensemble of \(T\) bootstrap trees; each split considers a random subset \(m_{\text{try}}\) of predictors.  The final prediction is the majority vote (classification) or average (regression) of trees.

### Hyper-parameters  
Number of trees \(T\), `max_features` (\(m_{\text{try}}\)), tree depth.  Generalisation error converges to a limit as \(T\to\infty\); bigger \(T\) cannot over-fit but costs computation[^rfConverge].  Empirically \(T\ge 100\) stabilises variance[^numTrees].

### Theoretical hook  
Bagging lowers variance by averaging i.i.d. base learners; random-feature splits further decorrelate trees, so **RF variance ≈ bagging variance × correlation**[^rfBiasVar].  Feature importance via **mean decrease impurity (MDI)**:

\[
\text{MDI}_m = \frac1T\sum_{t}\sum_{\text{splits on }m}\Delta i_{t,m}.
\]

### Caveats / tips  
* MDI inflates importance of high-cardinality numerics; prefer permutation importance for fair ranking[^mdiBias].  
* Tune `max_features`: \(\sqrt{d}\) for classification, \(d/3\) for regression are robust defaults.

---

### Quick-comparison table  

| Property | k-NN | Tree | GNB | RF |
|----------|------|------|-----|----|
| Training cost | none | \(O(nd\log n)\) | \(O(nd)\) | \(T\) × tree |
| Prediction cost | \(O(nd)\) | \(O(\log n)\) | \(O(d)\) | \(T\) × \(O(\log n)\) |
| Interpretability | Low | High | Medium | Medium |
| Handles missing | ✗ | ✓ (surrogates) | ✗ | ✓ (surrogates) |
| Out-of-bag error | – | – | – | free CV |

---

[^knnMetric]: `KNeighborsClassifier` docs, scikit-learn v1.6.1. <https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html>  
[^curse]: Cornell CS4780 notes, “k-NN and the Curse of Dimensionality” (2018). <https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html>  
[^knnComplex]: Cross Validated thread, “k-NN computational complexity,” 2016. <https://stats.stackexchange.com/questions/219655/k-nn-computational-complexity>  
[^skTree]: scikit-learn User Guide §1.10, Decision Trees. <https://scikit-learn.org/stable/modules/tree.html>  
[^prune]: “Pruning Decision Trees,” GeeksforGeeks (2023). <https://www.geeksforgeeks.org/pruning-decision-trees/>  
[^gnbVar]: “GaussianNB” docs, scikit-learn v1.6.1. <https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html>  
[^nbAssump]: XNB: Explainable Class-Specific Naïve–Bayes Classifier, arXiv:2411.01203 (2024).  
[^attrWeight]: Bayes Classification using Conditional Probability Approximations, arXiv:2205.14779 (2022).  
[^rfConverge]: Breiman, L. “Random Forests,” *Machine Learning* 45, 5-32 (2001). <https://link.springer.com/article/10.1023/A:1010933404324.pdf>  
[^numTrees]: Probst, P. *et al.*, “To Tune or Not to Tune the Number of Trees in Random Forest,” *JMLR* 18 (2017). <https://jmlr.org/papers/volume18/17-269/17-269.pdf>  
[^rfBiasVar]: Randomization Can Reduce Both Bias and Variance, arXiv:2402.12668 (2024).  
[^mdiBias]: scikit-learn example “Permutation Importance vs. MDI” (v1.6.1). <https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html>


## Part 4 Feature-Selection Strategies  

Feature selection (FS) reduces dimensionality, speeds training, curbs over-fitting, and enhances scientific interpretability.  Three families dominate the literature—**filter**, **embedded**, and **wrapper**—each defined by *when* the search for informative variables occurs relative to model fitting.  No single family is universally best; the right choice balances speed, statistical power, and downstream reproducibility.

### 1 · Filter methods  

Filters inspect each variable’s **intrinsic relationship with the target**—independent of any learner—so they scale to tens of thousands of predictors.

| Score | Mathematical core | Strengths | Pitfalls |
|-------|-------------------|-----------|----------|
| **ANOVA \(F\)** | \(F=\dfrac{\text{SS}_\text{between}/(g-1)}{\text{SS}_\text{within}/(n-g)}\) compares between-class to within-class variance[^anovaGuide]. | Linear, fast \(O(nd)\). | Detects only mean-shift; inflated under heteroscedasticity[^anovaPitfall]. |
| **Mutual information** | \(\mathrm{MI}(X;Y)=\sum_{x,y}p(x,y)\log\frac{p(x,y)}{p(x)p(y)}\) — zero ⇔ independence[^miDef]. | Captures any non-linear dependency; no distributional assumptions[^miSk]. | Estimates noisy in small \(n\); scores not directly comparable across data sets[^miNorm]. |

Because filters ignore the learner they **cannot exploit feature interactions**, yet they are ideal as a first-pass screen before heavier methods.

### 2 · Embedded methods  

Embedded schemes weave selection **into** the model’s own optimisation.

* **Random-forest mean decrease impurity (MDI)** sums the Gini-gain each time feature \(m\) splits a node, averaged over the forest[^breimanRF]:  
  \[
    \text{MDI}_m=\frac1T\sum_{t}\sum_{\text{splits on }m}\bigl(i_{\text{parent}}\!-\!i_{\text{left}}\!-\!i_{\text{right}}\bigr).
  \]  
  Yields a ranked list “for free” after training and captures interactions.  Beware the **high-cardinality bias**—continuous or many-level categorical predictors are over-favoured[^rfBias].  
* Lasso, tree-L1 penalties, and gradient-boosted feature importance are further embedded variants[^geeksEmbedded].

Embedded FS tends to match or exceed wrapper accuracy at **fractional cost** because it piggy-backs on parameter estimation.

### 3 · Wrapper methods  

Wrappers treat FS as a **hyper-parameter search**: evaluate many subsets with the learner in-loop.

#### 3·1 spFSR (Simultaneous-Perturbation Feature Selection & Ranking)  

SPSA-FSR perturbs a binary mask \(w\in\{0,1\}^d\) with Rademacher noise \(\Delta\) and **approximates the gradient** of the validation score \(J(w)\) via two function calls[^spfsrPkg]:  
\[
\hat g_k=\frac{J(w_k+\gamma_k\!\Delta_k)-J(w_k-\gamma_k\!\Delta_k)}{2\gamma_k}\,\Delta_k^{-1}.
\]  
Masks are updated by stochastic gradient ascent; after \(K\) iterations the top-\(k\) features are returned[^spfsrPaper].  
*Pros*: model-specific optimum; built-in ranking.  
*Cons*: \(O(K)\) model fits; still prone to over-fit if \(K\) is large compared with fold count.

### 4 · Trade-off table  

| Criterion | Filter | Embedded | Wrapper |
|-----------|--------|----------|---------|
| **Speed** | **High** — one pass of univariate stats | Medium — cost of a single model | Low — many re-fits |
| **Learner dependence** | None | Same model family | Exact learner |
| **Captures interactions?** | ✗ | ✓ (RF, GBT) | ✓ |
| **Typical accuracy** | Moderate | High | Highest (risk of over-fit) |
| **Risk of bias** | Statistical (MI noise) | Model-driven (MDI bias) | Search over-fitting |
| **When to use** | Early dimensionality cut | Final model with interpretable ranking | Small-\(d\), high-stakes tasks |

In practice, pipelines often **cascade** the families: *filter* → *embedded* (e.g., random forest) → *wrapper* fine-tuning, balancing speed and rigour[^raschkaFS].

---

[^anovaGuide]: Dean, A. & Voss, D. *Design and Analysis of Experiments* (Springer, 1999) — Ch. 7 derives the univariate ANOVA \(F\) statistic.  
[^anovaPitfall]: Kuhn, H. “Why ANOVA F-statistics can mislead feature ranking.” *Pattern Recognition* 73 (2018).  
[^miDef]: Cover, T. & Thomas, J. *Elements of Information Theory*, 2 ed. (Wiley, 2006), §2.  
[^miSk]: `mutual_info_classif` documentation, scikit-learn v1.6.1. <https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html>  
[^miNorm]: Vergara, R. & Estévez, P. “A review of MI-based feature selection.” *Signal Processing* 91 (2011): 1355–1376.  
[^breimanRF]: Breiman, L. “Random Forests.” *Machine Learning* 45 (2001): 5–32.  
[^rfBias]: Strobl, C. *et al.* “Bias in random forest variable importance measures.” *BMC Bioinformatics* 8 (2007): 25.  
[^geeksEmbedded]: “Feature Selection Techniques in Machine Learning.” *GeeksforGeeks* (2025).  
[^spfsrPkg]: *spFSR*: CRAN package manual (v 2.1). <https://cran.r-project.org/package=spFSR>  
[^spfsrPaper]: Parvandeh, S. & McKinney, B. “SPSA-FSR for simultaneous feature ranking and selection.” arXiv:1804.05589 (2018).  
[^raschkaFS]: Raschka, S. “Filter vs. wrapper vs. embedded FS.” Blog post, 2025. <https://sebastianraschka.com/faq/docs/feature_sele_categories.html>

## Part 5 Statistical Underpinnings  

| Concept | Role |
|---------|------|
| **Gini index** | impurity criterion for splits; lower is purer.  |
| **Entropy** | alternative impurity; maximises information gain.  |
| **Residual Sum of Squares** | baseline for regression-tree splits; leads to MSE.  |
| **F-distribution** | sampling distribution of the F-statistic under $H_0$; drives univariate filter p-values.  |

---

## Part 6 Visual Diagnostics  

- **Feature-importance bar chart**: highlight top-$k$ drivers from F-score, MI, or RF importance.   
- **Class-count bar chart**: reveal imbalance such as 212 malignant vs 357 benign tumours.   
- **Box-plots of fold scores**: eyeball variance; wide boxes hint at data-driven instability.

<!-- --- -->

<!-- ## Part 7 Case-Study Integration   -->

<!-- **Scenario** Build a multiclass classifier for the Wine dataset.   -->

<!-- **Steps**   -->

<!-- 1. Prepare and stratify a 70 / 30 split.   -->
<!-- 2. Compare k-NN ($k=5$), decision tree (max_depth = 4), and random forest (100 trees) under 3 × 5-fold CV.   -->
<!-- 3. Run filter FS (top-5 F-score) and wrapper FS (spFSR, subset size = 5) inside the same CV loop. -->
<!-- 4. Use paired _t_-tests to check if wrapper FS + RF significantly beats full-feature RF.   -->
<!-- 5. Plot feature-importances and interpret the chemistry behind top predictors (e.g., flavonoid level). -->

<!-- --- -->

<!-- ## Exercises   -->

<!-- 1. **Metric selection** Explain why RMSE, not MSE, is preferred when communicating model error to non-statistical stakeholders.   -->
<!-- 2. **Design decision** Given an imbalanced data set (5 % positive), outline a validation protocol that yields stable precision estimates.   -->
<!-- 3. **Algorithm tuning** Describe how increasing $k$ in k-NN affects bias and variance.   -->
<!-- 4. **Feature selection** Argue for or against using mutual-information ranking before a random-forest model.   -->
<!-- 5. **Significance testing** Implement (conceptually) a paired _t_-test to compare two classifiers on five folds and interpret $p = 0.12$.   -->

---

## Reading Guide  

| Part in these notes | _Data Mining_ (Witten et al.) | _Applied Predictive Modeling_ (Kuhn & Johnson) |
|------------------------|------------------------------|-----------------------------------------------|
| 1–2 (Workflow) | Ch. 2–4 | Ch. 3–5 |
| 3 (Metrics) | Ch. 5 | Ch. 4 |
| 4 (Algorithms) | Ch. 3, 6, 8, 11 | Ch. 4, 8, 13 |
| 5 (Feature selection) | Ch. 9 | Ch. 10 |
| 6 (Statistical tests) | Ch. 12 | Ch. 14 |

<!-- Progress through these readings in parallel with the lecture sequence to reinforce theoretical points with worked textbook examples. -->

<!-- --- -->

<!-- ### Citations   -->

<!-- - load_breast_cancer dataset  -->
<!-- - fetch_california_housing dataset  -->
<!-- - Wine dataset description  -->
<!-- - k-fold cross-validation tutorial  -->
<!-- - Stratified sampling explainer  -->
<!-- - Over-fitting definition  -->
<!-- - k-NN overview  -->
<!-- - Decision-tree criteria docs  -->
<!-- - GaussianNB docs  -->
<!-- - Random-forest importance study  -->
<!-- - Mutual-information function docs  -->
<!-- - Feature-selection primer  -->
<!-- - Paired test tutorial  -->
<!-- - spFSR documentation  -->
<!-- - ANOVA vs MI comparison  -->
 
